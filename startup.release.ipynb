{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f79a70a1-842d-5acd-d706-01fc7584fc0b"
   },
   "source": [
    "# Final Project:  Spam filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task\n",
    "\n",
    "You’re the project manager for an enterprise email system and assigned a task to develop a spam filter for a company's email system. We’ve collected email samples that have been validated to be spam or non-spam emails. Your task is to predicts whether an email contains spam or not.\n",
    "\n",
    "You are given 3068 training emails with two classes: \"spam\" or \"not spam\". Using these data, you are expected to build your own spam filter with the kownledge you learned from this course. The goal is to correctly classify 1292 test emails. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "- **Spam email**\n",
    "\n",
    "> Subject: younger and healthier with ultimate - hghl 7283  as seen on nbc , cbs , cnn , and even oprah ! the health discovery that actuallyreverses aging while burning fat , without dieting or exercise ! this provendiscovery has even been reported on by the new england journal of medicine . forget aging and dieting forever ! and it ' s guaranteed !  click below to enter our web site :  http : / / www . freehostchina . com / washgh /  would you like to lose weight while you sleep !  no dieting !  no hunger pains !  no cravings !  no strenuous exercise !  change your life forever !  100 % guaranteed !  1 . body fat loss 82 % improvement .  2 . wrinkle reduction 61 % improvement .  3 . energy level 84 % improvement .  4 . muscle strength 88 % improvement .  5 . sexual potency 75 % improvement .  6 . emotional stability 67 % improvement .  7 . memory 62 % improvement .  click below to enter our web site :  http : / / www . freehostchina . com / washgh /  if you want to get removed  from our list please email at - standardoptout @ x 263 . net ( subject = remove \" your email \" )\n",
    "\n",
    "- **Not-spam email**\n",
    "\n",
    "> Subject: december 6 th meeting  dear mr . kaminski :  this is to confirm the december 6 th meeting here at our center .  the location for the meeting is room # 3212 steinberg hall - dietrich hall and  the time will run from 9 : 00 am - 11 : 00 am .  please let us know if you need anything further .  we look forward to seeing you then .  regards ,  theresa convery  ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~  theresa convery  administrative assistant  risk and decision processes center  the wharton school of the university of pennsylvania  ( 215 ) 898 - 5688 / fax : ( 215 ) 573 - 2130  tconvery @ wharton . upenn . edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline of solution\n",
    "\n",
    "\n",
    "**1. Text representation.**\n",
    "\n",
    "As you can see, text content of emails is unstructured data. To apply machine learning methods on top of them, we first need to extract structured feature. To demonstrate this, we'll show using bag-of-word model for textural represention.\n",
    "\n",
    "** 2. Build your classifier. **\n",
    "\n",
    "As baseline, we provide solution based on *SVM* (Support Vector Machine).\n",
    "\n",
    "** 3. Evaluation **\n",
    "\n",
    "We will use *AP* (Average Precision) and *Accuracy* for performance evaluation in this notebook.\n",
    "\n",
    "Note that for evaluation on kaggle competition, [*MCE*](https://www.kaggle.com/wiki/MeanConsequentialError) (Mean Consequential Error) is used instead.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python package dependence\n",
    "- **pandas**   : for loading CSV files;\n",
    "- **nltk**     ：for word pre-processing;\n",
    "- **wordcloud**: for data visulization.\n",
    "\n",
    "Tips: To install missing packages, you can either do \"pip install package_name\" or \"conda install package_name\" in case of anaconda."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e2c1ada8-d68d-4e8d-e807-6e47ea7f5a58"
   },
   "source": [
    "# Data preparation\n",
    "\n",
    "## 1) Download data.\n",
    "\n",
    "Download \"emails.train.csv\", \"emails.test.csv\" from our kaggle competition page [here](https://www.kaggle.com/c/spamfilter-aml-uva/data), and put it under the same folder as this ipython notebook.\n",
    "\n",
    "\n",
    "\n",
    "## 2) Read in data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "e14bfbc8-92b8-4b60-081d-1f6fc0728ab1"
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np  # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "assert os.path.exists('./Data/emails.train.csv'), \"[Dataset File Not Found] Please download dataset first.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "204b6823-c7ea-8f47-a62b-f649f82c71ee"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Subject: naturally irresistible your corporate...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Subject: unbelievable new homes made easy  im ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Subject: 4 color printing special  request add...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Subject: do not have money , get software cds ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Subject: great nnews  hello , welcome to medzo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                               text  spam\n",
       "0   0  Subject: naturally irresistible your corporate...     1\n",
       "1   2  Subject: unbelievable new homes made easy  im ...     1\n",
       "2   3  Subject: 4 color printing special  request add...     1\n",
       "3   4  Subject: do not have money , get software cds ...     1\n",
       "4   5  Subject: great nnews  hello , welcome to medzo...     1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in csv file as dataframe\n",
    "df = pd.read_csv('./Data/emails.train.csv')\n",
    "\n",
    "# Show a snippet of dataset.\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, each emails has two fields: \n",
    "* \"text\": the full text content of an email.\n",
    "* \"spam\": an integer flag to mark whether an email is a spam (=1) or not (=0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================\n",
      "Example of spam emails\n",
      "---------------------------\n",
      "Subject: account zzzz @ example . com  new account for : zzzz @ example . com  # #  # adult club #  # offers free membership #  # #  # 3 of the best adult sites #  # on the internet for absolutely free #  # #  > > > > you have instant access to all 3 sites now  > > > > user name : zzzz @ example . com  > > > > password : 1534  - - - - - - - - - - - - - - - - - - - -  news 09 / 24 / 02  with just over 4 . 8 million members that signed up for free , last month there were 921 , 947 new  members . are you one of them yet ? ? ?  - - - - - - - - - - - - - - - - - - - -  our membership faq  q . why are you offering free access to 3 adult membership sites for free ?  a . i have advertisers that pay me for ad space so you don ' t have to pay for membership .  q . is it true my membership is for life ?  a . absolutely you ' ll never have to pay a cent the advertisers do .  q . can i give my account to my friends and family ?  a . yes , as long they are over the age of 18 .  q . do i have to sign up for all 3 membership sites ?  a . no just one to get access to all of them .  q . how do i get started ?  a . click on one of the following links below to become a member .  > > > > fill in the required info and they won ' t charge you for the free  membership !  > > > > if you don ' t believe us , just read their terms and conditions .  - - - - - - - - - - - - - - - - - - - -  # 3 . > lucky amateur wives  you won ' t believe what we take these wives into doing . . . free vip membership ! !  # 2 . > new ! just added today : cum drinkers  950 , 00 pics , 90 , 000 movies , live sex shows . . . free lifetime membership ! !  # 1 . > filthy teen sluts  the ultimate xxx teen site . . . free vip membership ! !  - - - - - - - - - - - - - - - - - - - -  jennifer simpson , miami , fl  your free lifetime membership has entertained my boyffriend and i for the last two years ! your  adult sites are the best on the net !  joe morgan manhattan , ny  your live sex shows and live sex cams are unbelievable . the best part about your porn sites , is  that they ' re absolutely free !  - - - - - - - - - - - - - - - - - - - -  disclaimer :  we are strongly against sending unsolicited emails to those who do not wish to receive our special  mailings . you have opted in to one or more of our affiliate sites requesting to be notified of any  special offers we may run from time to time . we also have attained the services of an independent  3 rd party to overlook list management and removal services . this is not unsolicited email . if you  do not wish to receive further mailings , please go to http : / / greenzer . com / remove . php to be removed  from the list . please accept our apologies if you have been sent this email in error . we honor all  removal requests .  thank you zzzz @ example . com  oldhtlheuhcclco\n",
      "===========================\n"
     ]
    }
   ],
   "source": [
    "print(\"===========================\")\n",
    "print(\"Example of spam emails\")\n",
    "print(\"---------------------------\")\n",
    "\n",
    "df_pos = df[df['spam']==1]\n",
    "\n",
    "print( np.random.choice(df_pos['text']) ) \n",
    "\n",
    "print(\"===========================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================\n",
      "Example of not spam emails\n",
      "---------------------------\n",
      "Subject: re : genetic programming  todd ,  we have several members of the group who can help you . one of them is steve  leppard located  in london .  vince  to : vince j kaminski / hou / ect @ ect  cc :  subject : genetic programming  vince -  i am with the ebs venture capital group . we are looking at evaluating an  investment in a company called widevine out of seattle . part of their value  proposition involves certain patents on genetic programming . are you familiar  with that ? if so , can i set a meeting with you and a few other members of my  group to discuss on tuesday or wednesday ?  todd van roten  enron broadband ventures  office : ( 713 ) 853 - 3850  fax : ( 713 ) 646 - 8010  cell : ( 713 ) 305 - 0110\n",
      "===========================\n"
     ]
    }
   ],
   "source": [
    "print(\"===========================\")\n",
    "print(\"Example of not spam emails\")\n",
    "print(\"---------------------------\")\n",
    "\n",
    "df_neg = df[df['spam']==0]\n",
    "\n",
    "print( np.random.choice(df_neg['text']) ) \n",
    "\n",
    "print(\"===========================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text representation\n",
    "\n",
    "## 1) Create the bags of words vocabulary. \n",
    "\n",
    "The bag-of-words model is a simplifying representation used in natural language processing and information retrieval (IR). In this model, a text (such as a sentence or a document) is represented as the bag (multiset) of its words, disregarding grammar and even word order but keeping multiplicity. The bag-of-words model has also been used for computer vision. (from [wikipedia](https://en.wikipedia.org/wiki/Bag-of-words_model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word         frequency\n",
      "-------------------------\n",
      "-                72932\n",
      ".                65233\n",
      ",                42792\n",
      "the              34362\n",
      ":                29814\n",
      "to               28822\n",
      "/                27959\n",
      "and              19173\n",
      "of               16317\n",
      ">                14658\n",
      "a                13894\n",
      "you              13305\n",
      "in               12631\n",
      "i                12151\n",
      "for              11639\n",
      "...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def select_vocabulary(dataframe, topN=100):\n",
    "    # for w in dataframe.str.lower():\n",
    "    word2freq = dict()\n",
    "    for line in dataframe:\n",
    "        words = line.split()\n",
    "        for word in words:\n",
    "            word2freq[word] = word2freq.setdefault(word, 0) +1\n",
    "\n",
    "    word_freq = [ (word,freq) for word,freq in word2freq.iteritems() ]\n",
    "    \n",
    "    # sort according to freq, in descending order.\n",
    "    word_freq.sort(key=lambda x:x[1], reverse=True) \n",
    "    \n",
    "    # show selection results\n",
    "    print(\"%-10s  %10s\" % ('word', 'frequency'))\n",
    "    print(\"-------------------------\")\n",
    "    for i in range(15):\n",
    "        print(\"%-10s  %10d\" % (word_freq[i]))\n",
    "    print(\"...\\n\")\n",
    "    return([x[0] for x in word_freq[:topN]])\n",
    "\n",
    "vocabulary = select_vocabulary(df['text'])\n",
    "word2ind   = dict(zip(vocabulary, range(len(vocabulary))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Extract Bag-of-Word Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-', '.', ',', 'the', ':', 'to', '/', 'and', 'of', '>', 'a', 'you', 'in', 'i', 'for', '_', '@', 'enron', 'is', 'on', 'ect', ')', \"'\", 'this', '?', '(', 'be', 'that', 'your', '\"', 'with', 'we', 'vince', '*', 'will', 'have', 'at', 'from', 'it', 'are', 's', 'as', 'Subject:', 'hou', 'com', 'by', 'or', 'if', '!', 'am', '2000', 'please', 'kaminski', 'subject', 'me', 'would', 'not', 'our', 'can', 're', '$', 'cc', 'my', 'j', 'an', '1', '=', 'pm', '2', '2001', 'all', 'research', 'any', 'time', '10', 'do', 'thanks', 'know', ';', 'has', '3', 'may', 'information', 'group', 'about', 'e', 'new', 'he', '0', 'energy', 'was', 'more', 'get', 'like', '00', 'business', 'risk', 'one', 'but', 'up']\n"
     ]
    }
   ],
   "source": [
    "print vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting feature for train ...\n",
      "   0 / 4021 \n",
      "1000 / 4021 \n",
      "2000 / 4021 \n",
      "3000 / 4021 \n",
      "4000 / 4021 \n",
      "4020 / 4021 \n",
      "Extracting feature for test ...\n",
      "   0 / 1707 \n",
      "1000 / 1707 \n",
      "1706 / 1707 \n",
      "Finish.\n"
     ]
    }
   ],
   "source": [
    "def extract_Bag_of_Word_feature(dataframe):\n",
    "    BoWs = np.zeros((len(dataframe), len(vocabulary)), dtype=np.float32)\n",
    "\n",
    "    for i, line in enumerate(dataframe):\n",
    "        for word in line.split():\n",
    "            word_ind = word2ind.get(word, -1)\n",
    "            if(word_ind>=0):\n",
    "                BoWs[i, word_ind] += 1\n",
    "\n",
    "        if i%1000==0:\n",
    "            print(\"%4d / %d \" % (i, len(dataframe)))\n",
    "    print(\"%4d / %d \" % (i, len(dataframe)))\n",
    "            \n",
    "    return BoWs\n",
    "\n",
    "\n",
    "# Make sure use cleaned version\n",
    "train = pd.read_csv('./Data/emails.train.csv')\n",
    "test  = pd.read_csv('./Data/emails.test.csv')\n",
    "\n",
    "# Get labels\n",
    "Y_train = train['spam']\n",
    "Y_test  = test['spam']\n",
    "\n",
    "print(\"Extracting feature for train ...\")\n",
    "X_train = extract_Bag_of_Word_feature(train['text'])\n",
    "\n",
    "print(\"Extracting feature for test ...\")\n",
    "X_test  = extract_Bag_of_Word_feature(test[ 'text'])\n",
    "\n",
    "print('Finish.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Performance of: SVM =======\n",
      "Metric[Accuracy            ]  0.915056\n",
      "Metric[Average Precision   ]  0.938935\n",
      "[output] Data/solution.SVM.csv\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import average_precision_score, accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "model = SVC()\n",
    "\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "def eval(model, X_test, Y_test, method=''):\n",
    "    print(\"====== Performance of: {method} =======\".format(method=method))\n",
    "    \n",
    "    # Predict decision labels.\n",
    "    Y_pred  = model.predict(X_test)  \n",
    "    print(\"Metric[{metric:20s}]  {score:-3f}\".format( metric=\"Accuracy\", \n",
    "                                              score=accuracy_score(Y_test, Y_pred)) )\n",
    "\n",
    "    # Predict confidence scores.\n",
    "    Y_score = model.decision_function(X_test)    \n",
    "    print(\"Metric[{metric:20s}]  {score:-3f}\".format( metric=\"Average Precision\", \n",
    "                                              score=average_precision_score(Y_test, Y_score)) )\n",
    "\n",
    "    # write to submit format\n",
    "    outf = 'Data/solution.%s.csv'% method\n",
    "    with open( outf, 'w') as f:\n",
    "        f.write('id,spam\\n')\n",
    "        for i in range(len(Y_pred)):\n",
    "            # print test['id'][0]\n",
    "            f.write('%s,%s\\n' % (test['id'][i], Y_pred[i]) )\n",
    "    print(\"[output] \"+outf)\n",
    "    \n",
    "    \n",
    "# evaluate current model\n",
    "eval(model, X_test, Y_test, method='SVM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the positive and negative examples in test is not balance (with Nr(pos)=1707, Nr(neg)=415 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Predict as all negative =======\n",
      "Metric[Accuracy            ]  0.756883\n",
      "====== Predict by random guess =======\n",
      "Metric[Average Precision   ]  0.227261\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Predict all labels as negative (=0)\n",
    "print(\"====== Predict as all negative =======\")\n",
    "Y_pred_all_neg  = np.zeros((len(Y_test),), dtype=np.int)\n",
    "print(\"Metric[{metric:20s}]  {score:-3f}\".format( metric=\"Accuracy\", \n",
    "                                          score=accuracy_score(Y_test, Y_pred_all_neg)) )\n",
    "\n",
    "# Random guess performance\n",
    "print(\"====== Predict by random guess =======\")\n",
    "Y_score_rand = np.random.uniform(0,1, (len(Y_test),))    # Generate prediction score by random\n",
    "print(\"Metric[{metric:20s}]  {score:-3f}\".format( metric=\"Average Precision\", \n",
    "                                              score=average_precision_score(Y_test, Y_score_rand)) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Advanced: Data Preprocess\n",
    "\n",
    "However, obtaining good textural representation can be tricky as you may notice that the content of emails are noisy.\n",
    "We'll provide example code for text cleaning and you are expected to come up with smarter way to do it.\n",
    "\n",
    "## Text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named wordcloud",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-cfbcbb7d5d39>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'matplotlib inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mwordcloud\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mwordcloud\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named wordcloud"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "def wordcloud(dataframe, title=None):\n",
    "    wordcloud = WordCloud(background_color=\"white\").generate(\" \".join([i for i in dataframe.str.upper()]))\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis(\"off\")\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "\n",
    "# show the word cloud of orignial dataset.\n",
    "wordcloud(df['text'], 'orignial dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### **Stop words** \n",
    " Stop Words are words which do not contain important significance to be used in Search Queries. For example, 'a', 'the', 'is', 'as', etc. Usually these words need to be filtered out because they return vast amount of unnecessary information. \n",
    "\n",
    "- ### **Stemming**\n",
    " In linguistic morphology and information retrieval, stemming is the process for reducing inflected (or sometimes derived) words to their stem, base or root form—generally a written word form. The stem need not be identical to the morphological root of the word; it is usually sufficient that related words map to the same stem, even if this stem is not in itself a valid root. Algorithms for stemming have been studied in computer science since the 1960s. Many search engines treat words with the same stem as synonyms as a kind of query expansion, a process called conflation.\n",
    "\n",
    " Stemming programs are commonly referred to as stemming algorithms or stemmers.\n",
    "\n",
    "- ### **Lemmatization**\n",
    " Lemmatisation (or lemmatization) in linguistics, is the process of grouping together the different inflected forms of a word so they can be analysed as a single item.\n",
    "\n",
    " In computational linguistics, lemmatisation is the algorithmic process of determining the lemma for a given word. Since the process may involve complex tasks such as understanding context and determining the part of speech of a word in a sentence (requiring, for example, knowledge of the grammar of a language) it can be a hard task to implement a lemmatiser for a new language.\n",
    "\n",
    " In many languages, words appear in several inflected forms. For example, in English, the verb ‘to walk’ may appear as ‘walk’, ‘walked’, ‘walks’, ‘walking’. The base form, ‘walk’, that one might look up in a dictionary, is called the lemma for the word. The combination of the base form with the part of speech is often called the lexeme of the word.\n",
    "\n",
    " Lemmatisation is closely related to stemming. The difference is that a stemmer operates on a single word without knowledge of the context, and therefore cannot discriminate between words which have different meanings depending on part of speech. However, stemmers are typically easier to implement and run faster, and the reduced accuracy may not matter for some applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ll use python package *nltk* to do this. But before any operation, we need to download necessary nltk corpuses first with its interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download nltk corpus\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Instal Wordnet corpus                | Instal Stopwords corpus                |\n",
    "| ------------------------------------ |:--------------------------------------:|\n",
    "| ![alt text](images/nltk_wordnet.png) | ![alt text](images/nltk_stopwords.png) |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= Clearn tranining data ==========\n",
      "Performing: lemm ...\n",
      "Performing: stem ...\n",
      "Performing: filtering ...\n",
      "========= Clearn testing data ==========\n",
      "Performing: lemm ...\n",
      "Performing: stem ...\n",
      "Performing: filtering ...\n",
      "finish\n"
     ]
    }
   ],
   "source": [
    "def text_regularize(dataframe, method='lemm'):\n",
    "    print('Performing: %s ...' % method)\n",
    "    def stemming(worker, tag):\n",
    "        return worker.stem(tag)\n",
    "\n",
    "    def lemmatize(worker, tag):\n",
    "        return worker.lemmatize(tag)\n",
    "\n",
    "    if   method=='stem':\n",
    "        worker = nltk.PorterStemmer()\n",
    "        func = stemming\n",
    "    elif method=='lemm':\n",
    "        worker = nltk.WordNetLemmatizer()\n",
    "        func = lemmatize\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    for i, line in enumerate(dataframe['text']):\n",
    "        elems = line.strip().split()\n",
    "\n",
    "        # apply stemming or lemmatize\n",
    "        newtags = [func(worker,tag.lower()) for tag in elems]\n",
    "        newline = \" \".join(newtags)\n",
    "\n",
    "        # update text\n",
    "        dataframe.loc[i,'text'] = newline\n",
    "\n",
    "    # return dataframe\n",
    "\n",
    "def text_filtering(dataframe, extras=set()):\n",
    "    print('Performing: filtering ...')\n",
    "    import re\n",
    "    \n",
    "    from nltk.corpus import stopwords\n",
    "    filter_set = set(stopwords.words('english'))\n",
    "    filter_set.update(extras)\n",
    "\n",
    "    for i, line in enumerate(dataframe['text']):\n",
    "        # remove special characters with regex\n",
    "        line = re.sub(r'[^\\w]', ' ', line)\n",
    "\n",
    "        # remove digits with regex\n",
    "        line = re.sub(\"(^|\\W)\\d+($|\\W)\", \" \", line)\n",
    "\n",
    "        # remove stop words\n",
    "        elems = line.strip().split()\n",
    "        newtags = filter(lambda x: x not in filter_set, elems)\n",
    "        newline = \" \".join(newtags)\n",
    "\n",
    "        # update text\n",
    "        dataframe.loc[i, 'text'] = newline\n",
    "    # return dataframe\n",
    "    \n",
    "\n",
    "print('========= Clearn tranining data ==========')\n",
    "# Read in training data\n",
    "df = pd.read_csv('./Data/emails.train.csv')\n",
    "\n",
    "# Do cleaning\n",
    "text_regularize(df, 'lemm')\n",
    "text_regularize(df, 'stem')\n",
    "text_filtering(df, extras=set(['subject', 'ect', 'hou', '_']))\n",
    "\n",
    "# Save as new file\n",
    "df.to_csv('Data/emails_clean.train.csv')\n",
    "\n",
    "\n",
    "print('========= Clearn testing data ==========')\n",
    "# Read in testing data\n",
    "df = pd.read_csv('./Data/emails.test.csv')\n",
    "\n",
    "# Do cleaning\n",
    "text_regularize(df, 'lemm')\n",
    "text_regularize(df, 'stem')\n",
    "text_filtering(df, extras=set(['subject', 'ect', 'hou', '_']))\n",
    "\n",
    "# Save as new file\n",
    "df.to_csv('Data/emails_clean.test.csv')\n",
    "\n",
    "print('finish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wordcloud' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-a682d8ce6b92>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Now show the word cloud after cleaning.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mwordcloud\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'After text cleaning'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Based on the observation from this word cloud, you may add more non-meaningful words into `extras'.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wordcloud' is not defined"
     ]
    }
   ],
   "source": [
    "# Now show the word cloud after cleaning.\n",
    "wordcloud(df['text'], 'After text cleaning')\n",
    "\n",
    "# Based on the observation from this word cloud, you may add more non-meaningful words into `extras'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TODO] Preparing text feature.\n",
      "[TODO] Training your model.\n",
      "[TODO] Evaluate your model.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train = pd.read_csv('./emails_clean.train.csv')\n",
    "test  = pd.read_csv('./emails_clean.test.csv')\n",
    "\n",
    "# Get labels\n",
    "Y_train = train['spam']\n",
    "Y_test  = test[ 'spam']\n",
    "\n",
    "\n",
    "print(\"[TODO] Preparing text feature.\")\n",
    "#############################\n",
    "#####  Your solution    #####\n",
    "#############################\n",
    "\n",
    "\n",
    "\n",
    "print(\"[TODO] Training your model.\")\n",
    "#############################\n",
    "#####  Your solution    #####\n",
    "#############################\n",
    "\n",
    "\n",
    "\n",
    "print(\"[TODO] Evaluate your model.\")\n",
    "#############################\n",
    "#####  Your solution    #####\n",
    "#############################\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "_change_revision": 2,
  "_is_fork": false,
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
