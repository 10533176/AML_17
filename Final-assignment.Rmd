---
title: "Predicting spam from e-mail content"
output:
  html_notebook: default
  pdf_document: default
---

# Introduction

Most people are familiar with spam e-mails. They pop up in our mailbox almost every month, maybe every day. Spam mails can be an annoying distraction and, in some cases, bring virusses to computers. We can install spam filters to counter these spam mails. In this essay I will make the case for my own spam filter, because regocnizing spam is important for the safety of our computers. I will try to predict the classification of the e-mails (Spam vs. non-Spam) in regard to the words and punctuation used in the spam mails.

The implications of a statistical learning method is the fact that we can apply this model to incoming mails and automatically direct them to a spam folder. If the model has a high accuracy, this method is highly applicable for implementation in mailboxes to classify e-mails as spam.

# Data

We will use the **Spambase** dataset that can be found at [https://www.dropbox.com/s/nts26or7i7jk0bw/spamnames.names?dl=0], this file requires the corresponding names file that can be found at [https://www.dropbox.com/s/3341twazs1an8un/spambase.data?dl=0].

```{r load data and preparation}
library(data.table)
data<-fread('spambase.data')
names<-fread('spamnames.names',
             header=FALSE)
names<-names$V1
colnames(data)<-c(names,'Spam')
```

This data set features a set of variables representing the frequencies of words and symbols in 4601 e-mails collected.

In the dataset are 48 continuous [0,100] attributes of type word_freq_WORD, which is the percentage of words in the e-mail that match WORD.
$$
\text{WORD_frequency} = \frac{\text{100 * number of times the WORD appears in the email}}{\text{total number of words in email}}
$$
Alongside these, there are 6 continuous real [0,100] attributes of type char_freq_CHAR, which is the percentage of characters in the e-mail that match CHAR.
$$
\text{CHAR_frequency} = \frac{\text{100 * number of CHAR occurences}}{\text{total characters in e-mail}}
$$
Furthermore, there are 3 continuous attributes at the end, which represent:

- average length of uninterrupted sequences of capital letters.
- length of longest uninterrupted sequence of capital letters.
- total number of capital letters in the e-mail

Finally, there is 1 nominal {0,1} class attribute of type spam, which denotes whether the e-mail was considered spam (1) or not (0).This is the target variable.

```{r, echo=FALSE}
head(data)
```


# Methods

I will compare two methods for classification in two groups: Linear discriminant analysis and dicision trees. I chose for the former because there might be a linear combination of features in the dataset which characterizes or separates two classes. If that would be so, we can use the resulting combination as a linear classifier. It is highly possible that all the features in the dataset are linearly related to the classification of spam, because the dataset mostly includes words and characters that are not commonly used in personal e-mails, like dollarsigns and exclamation marks.
My motivation for the latter technique is that it describes data very accurately, because all of the variables are used as splitting variables. It is simple to understand and interpret, is robust and performs extremely well with large datasets. It is very well possible that a decision tree model can make accurate predictions about spam mail.

I will evaluate the performance of each model by assessing the accuracy of each model on the validation data, where accuracy is defined as:
$$
\text{accuracy} = \frac{\text{# correctly classified examples}}{\text{# total examples}}
$$

First, we need to split the data into a training set and a validation set.

```{r}
set.seed(1)
# Split data into train data and valid data
in_train = sample(
  c(T,F),
  nrow(data),
  TRUE,
  c(0.8, 0.2))
train_data = data[in_train, ]
valid_data = data[!in_train,]
```
## LDA Classifier

To perform an LDA, we first have to train an LDA model on our training data.

```{r}
library(MASS)
fit<-lda(Spam~.,data= train_data)
```
We can visualize if the LDA makes sense by checking whether the histograms for the predictions differ.

```{r}
pred<-predict(fit,valid_data)
ldahist(pred$x,g=valid_data$Spam)
```
To compute our accuracy, we use the following code:

```{r}
table(pred$class,valid_data$Spam)

AA<-sum(
  diag(
    prop.table(
      table(pred$class,valid_data$Spam)
    )
  )
)

print(AA)
```
Alright, so that is pretty good. The LDA classifier predicts 88% of the mails in the validation data right.

There is of course a cross-validation function for the LDA. Let's compute the accuracy for the cross-validated LDA classifier.
```{r}
fit.cv<-lda(Spam~.,data=train_data,CV=TRUE)
table(fit.cv$class,train_data$Spam)
sum(diag(prop.table(table(fit.cv$class, train_data$Spam))))
```
As you can see, it is almost the same as the normal LDA classifier. This means the LDA has an overall accuracy of 0.88.

## Decision tree models

Now let's compare the accuracy of the LDA classifier with that of the decision tree model. There are numerous types of decision tree models, so first we have to find out which decision tree model is the best.

Let's first define a normal decision tree model.

```{r}
library(tree)
library(rpart)
# Fit decision tree model
decision_tree_model = rpart(Spam ~ .,
                            train_data,
                            method = "class")
decision_tree_model
```

```{r}
library(rpart.plot)

# Visualize decision tree model
rpart.plot(decision_tree_model,
           type = 1,
           cex=0.7)
```
To examine if the predictions are somewhat right, we can look at a sample of the validation data of which we know that they are spam or not. We select 2 e-mails that are spam and 2 that are not spam from the validation data.
```{r}
valid_is_spam = valid_data[,valid_data$Spam] == 1
valid_spam = which(valid_is_spam)
valid_not_spam = which(!valid_is_spam)
sample_valid_data_not_spam = valid_not_spam[21:22]
sample_valid_data_spam = valid_spam[7:8]
sample_valid_data = valid_data[c(sample_valid_data_spam, sample_valid_data_not_spam),]
sample_valid_data
```
This is the true value of the spam variable in our sample. As we see, the first two are spam and the second two are not.
```{r}
sample_valid_data$Spam
```

Let's see what our decision tree model predicts on these e-mails.
```{r}
predict(decision_tree_model,
        newdata = sample_valid_data,
        type='class')
```
And compute the accuracy of this prediction.
```{r}
pred<-predict(decision_tree_model,
              newdata = sample_valid_data,
              type='class')

sum(
  diag(
    prop.table(table(pred,sample_valid_data$Spam))
  )
)
```
This accuracy does not seem high, then again we have only predicted 4 values. Here, we compute the accuracy of the decision tree model on the validation data.
```{r}
# Accuracy decision tree model
prediction<-predict(decision_tree_model,
                    newdata = valid_data,
                    type = 'class')

table('pred'= prediction,
      'real'= valid_data$Spam)

# Accuracy
A<-sum(
  diag(
    prop.table(
      table(
        'pred'= prediction,
        'real'= valid_data$Spam)
      )
    )
  )

print(A)
```
Because we didn't specify what the maximum depth (the amount of splits) in our model was, we restricted the model to the default depth. To find out if a model with less splits is better at predicting the data, we will fit a model with a maximum depth of 2 splits.
```{r}
small_model = rpart(Spam ~ ., train_data,
                    control = rpart.control(
                      maxdepth = 2)
                    )
rpart.plot(small_model,
           type = 1,
           cex=0.7)
```

We compute the accuracy for this model in the same way as before.
```{r}
pred_small<-predict(small_model,
                    newdata = valid_data)

pred_small<-ifelse(pred_small<0.5,
                   yes = 0,
                   no = 1
                   )

table('Pred'=pred_small,
      'Real'= valid_data$Spam)

# Accuracy
B<-sum(
  diag(
    prop.table(
      table(
        pred_small,
        valid_data$Spam)
      )
    )
  )

print(B)
```

This is actually worse than our first decision tree. But no worries, there is another option. We can specify a model with a large maximum depth, allowing the tree to go very deep. Since our smaller model gave us a lower accuracy, let's try a big model.

```{r}
big_model = rpart(Spam ~ .,
                  train_data,
                  method = "class",
                  control=rpart.control(
                    cp=0.00025))

rpart.plot(big_model,
           type = 1,
           cex=0.7)
```


And compute it's accuracy.
```{r}
pred<-predict(big_model,
              newdata = valid_data,
              type='class')

table(pred,
      valid_data$Spam)

# Accuracy
D<-sum(
  diag(
    prop.table(
      table(
        pred,
        valid_data$Spam)
      )
    )
  )

print(D)
```

The accuracy of the big model is better than our first decision tree and way better than our small model.

A third type of decision tree is the boosted decision tree. Boosting builds the model in a stage-wise fashion like other boosting methods do, and it generalizes them by allowing optimization of an arbitrary differentiable loss function.

```{r}
library(gbm)
set.seed(1)

decision_boost_model = gbm(Spam ~ ., train_data,
                           distribution = "gaussian",
                           n.trees = 1000,
                           shrinkage = 0.01,
                           interaction.depth = 4)

decision_boost_model
```

We can now look at what the most important variables (word or characters) are in the classification of spam. An exclamation mark and a dollar sign seem to be the most important.
```{r}
par(mar = par('mar')+diag(4,4)[,2]) # make the left margin wider
summary(decision_boost_model,
        las=1,
        cex.names=0.7)
```

Let's evaluate the accurac of the decision boost model on the validation data.
```{r}
link = predict(decision_boost_model,
               valid_data,
               n.trees = 1000)

pred.boost = ifelse(link < 0,
                    yes = 0,
                    no = 1)

table(pred.boost,
      valid_data$Spam)

# Accuracy
C<-sum(
  diag(
    prop.table(
      table(
        pred.boost,
        valid_data$Spam)
      )
    )
  )

print(C)
```

That's not good at all! It is even worse than our small model at the time.

Luckely, we have more options for decision trees. A random forest classifier uses a number of decision trees in order to improve the classification rate. We can try if fitting a random forest model to our data makes the accuracy of the model better than the accuracy of our big model (and the LDA classifier).

```{r}
library(randomForest)
set.seed(2)

randomfor<-randomForest(Spam ~.,
                        data = data.frame(
                          train_data)
                        )

print(randomfor)
```
```{r}
plot(randomfor)
varImpPlot(randomfor)
```

```{r}
importance(randomfor)
```

What is the accuracy of the random forest model?

```{r}
pred<-predict(randomfor,
              newdata = data.frame(
                valid_data),
              type='class')

pred<-ifelse(pred<0.5,
             yes = 0,
             no = 1)

table(pred,
      valid_data$Spam)

E<-sum(
  diag(
    prop.table(
      table(
        pred,
        valid_data$Spam)
      )
    )
  )

print(E)
```
That is really high. Our random forest model predicts 95% of the e-mails correst as spam or no spam. That is almost as high as a professional spam filter.

There is one more technique we can try to improve our accuracy. This is called a rotation forest model and is not covered in the course. A rotation forest model is a model in which every decision tree is trained by first applying principal component analysis (PCA) on a random subset of the input features.

Let's fit a rotation forest model to the train data.
```{r}
library(rotationForest)
set.seed(2)

y<-as.factor(train_data$Spam)
x<-data.frame(train_data)[,-58]

rotation_forest<-rotationForest(x = x,y=y)
```

And compute it's accuracy on the validation data.

```{r}
newX<-data.frame(valid_data)[,-58]
pred<- predict(rotation_forest,newdata = newX)
pred<-ifelse(pred<0.5,yes=0,no=1)
table('pred'=pred,'real'=valid_data$Spam)

H<-sum(
  diag(
    prop.table(
      table('pred'=pred,
            'real'=valid_data$Spam)
    )
  )
)
print(H)
```

Unfortunately, it is worse than our random forest model. We will try to explain this later on in the discussion.

For now, this is a summary of the accuracy for every decision tree model.
As we can see, the accuracy of the random forest model is the highest.

```{r, echo=FALSE}
data.frame(
  'Decision_boost_model' = C,
  'Small_model' = B,
  'Decision_tree_model' = A,
  'Rotation_forest'=H,
  'Big_model'= D,
  'Random_forest'= E,
  row.names = 'Accuracy'
)
```

# Results

Here, we compare the highest accuracy achieved with both methods. The best predictions come from the random forest model.
```{r, echo=FALSE}
data.frame(
  'LDA'=AA,
  'Random_Forest'=E,
  row.names='Accuracy'
)
```

This is not to say that the random forest model predicts every mail right.
These are all the wrong predictions by the random forest model, the model predicted that these e-mails were spam when the really were not.

```{r}
valid_data[which(pred== 1 & valid_data$Spam == 0)]
```

# Conclusion